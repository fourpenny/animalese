{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd150d9-9df0-490b-86c6-54e100f69b75",
   "metadata": {},
   "source": [
    "# Animalese: An Animal Crossing Dialogue Generator\n",
    "The Animal Crossing series of video games is one of the most popular in the world. One of the biggest draws of the games is their extensive dialogue: each character appears to have its own personality, despite them drawing from a script of limited text. However, long time players can sometimes become frustrated as eventually this dialogue becomes repetitive. Because of the large amount of available source material, this seemed like a great opportunity to apply neural networks to generate \"new\" dialogue from the games.\n",
    "\n",
    "Neural networks can generate truly new sequences of text rather than procedurally generated, scripted variations of existing phrases. There are essentially unlimited future results based on the unlimited set of inputs you can provide a network. In this notebook, we will try to generate comprehensible English text in the style of Animal Crossing dialogue by using a RNN (recurrent neural network). These types of neural networks are used to predict sequential data, such as English text.\n",
    "\n",
    "Our first step in creating a neural network is obtaining existing data to train it on. In this case, this is lines of dialogue from the Animal Crossing series. Because none of the scripts are publicly available, I instead use a web scraper to retrieve fan transcribed lines available online and process these pages of dialogue for use in model training. I then process this text for use in two different RNN models, build said models, then use both to generate new dialogue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec224dc-19a2-467f-9805-15ebc61547d4",
   "metadata": {},
   "source": [
    "## Obtaining text through web scraping\n",
    "\n",
    "Here are the packages that we will need to scrape the webpages that contain lines of dialogue and then save them for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ed757c-16d6-4c6c-b5ac-6464fe56a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77990078-5be3-423c-8f25-accb513527a8",
   "metadata": {},
   "source": [
    "Here are links to the sites where we'll be downloading the dialogue from, the Animal Crossing fandom.com site and Nookipedia, an Animal Crossing wiki. Because the formatting of these pages can vary slightly, there are a few sites that are put in a second list that we will process slightly differently than the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a99c65a0-cedc-4bf1-9570-726aaad18904",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = [\"https://animalcrossing.fandom.com/wiki/Guide:Cranky_dialogues_(New_Leaf)\", \n",
    "            \"https://animalcrossing.fandom.com/wiki/Guide:Peppy_dialogues_(New_Leaf)\",\n",
    "            \"https://animalcrossing.fandom.com/wiki/Guide:Player_dialogues\",\n",
    "            \"https://animalcrossing.fandom.com/wiki/Guide:Normal_dialogues_(New_Leaf)\",\n",
    "            \"https://animalcrossing.fandom.com/wiki/Guide:Lazy_dialogues_(New_Leaf)\", \n",
    "            \"https://animalcrossing.fandom.com/wiki/Guide:Sisterly_dialogues_(New_Leaf)\",\n",
    "            \"https://animalcrossing.fandom.com/wiki/Guide:Jock_dialogues_(New_Leaf)\", \n",
    "            \"https://animalcrossing.fandom.com/wiki/Guide:Smug_dialogues_(New_Leaf)\",\n",
    "            \"https://animalcrossing.fandom.com/wiki/Guide:Snooty_dialogues_(New_Leaf)\",\n",
    "            \"https://nookipedia.com/wiki/Cranky/New_Horizons_dialogue\",\n",
    "            \"https://nookipedia.com/wiki/Lazy/Wild_World_dialogue\",\n",
    "            \"https://nookipedia.com/wiki/Lazy/Pocket_Camp_dialogue\"]\n",
    "\n",
    "label_list = [\"cranky\",\"peppy\",\"player\",\"normal\",\"lazy\",\"uchi\",\"jock\",\"smug\",\"snooty\",\"cranky\",\"lazy\",\"lazy\"]\n",
    "#the format of these pages is slightly different, so they are their own list\n",
    "p_urls = [\"https://animalcrossing.fandom.com/wiki/Guide:Isabelle_dialogues\",\"https://animalcrossing.fandom.com/wiki/Guide:Resetti_dialogues_(Animal_Crossing)\",\n",
    "\"https://animalcrossing.fandom.com/wiki/Franklin_Dialogue_(GCN)\", \"https://animalcrossing.fandom.com/wiki/Jingle_Dialogue_(GCN)\"]\n",
    "p_labels = [\"isabelle\",\"resetti\",\"franklin\",\"jingle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c3b67a-7de9-441d-9703-27edbba0db1c",
   "metadata": {},
   "source": [
    "We'll be storing the scraped data in two lists that can later be stored as a Pandas dataframe and exported as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9480cffb-788c-4f6c-b527-e721af9d0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4491a-6d1e-4f6d-9e01-b0acef51225d",
   "metadata": {},
   "source": [
    "We will use this regular expression to help clean the text obtained from each webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6346f2dc-44ee-40c0-b01c-54ea759d92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"\\\"([\\S+\\s]+)\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f12c8-ad1a-4879-8ef1-9a578e9ba9f9",
   "metadata": {},
   "source": [
    "Here we'll scrape the first group of URLs where the dialogue is contained in li tags on each webpage. We'll get the page's html content using the requests package, then parse it using the BeautifulSoup4 package. After we parse the page's text content contained in the list item tags, we clean it using the regular expression above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1680cae2-6509-43b6-9ccb-0355f258a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scraping first set of URLs!\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(url_list)):\n",
    "    page = requests.get(url_list[j])\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    li = soup.find_all('li')\n",
    "    for item in li:\n",
    "        for i in item.children:\n",
    "            if i.string != None:\n",
    "                text = i.string\n",
    "                #use regex to clean up the string\n",
    "                clean = pattern.match(text)\n",
    "                if clean != None:\n",
    "                    dialogue.append(clean.group(1))\n",
    "                    labels.append(label_list[j])\n",
    "                    \n",
    "print(\"Done scraping first set of URLs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3952e-cf99-42f3-8a09-d26da431d502",
   "metadata": {},
   "source": [
    "Now we will do the same with the second group of URLs where the dialogue is contained in paragraph tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b70a69-38bf-41ea-9897-6c22f7cb6d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scraping second set of URLs!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(p_urls)):\n",
    "    page = requests.get(p_urls[i])\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    p = soup.find_all('p')\n",
    "    for paragraph in p:\n",
    "        if paragraph.string != None:\n",
    "            text = paragraph.string\n",
    "            #use regex to clean up the string\n",
    "            clean = pattern.match(text)\n",
    "            if clean != None:\n",
    "                dialogue.append(clean.group(1))\n",
    "                labels.append(p_labels[i])\n",
    "\n",
    "print(\"Done scraping second set of URLs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b790309-0c4f-4410-b17e-3ef4731b6862",
   "metadata": {},
   "source": [
    "Now we store everything in a Pandas DataFrame and export it as a .csv file for future use if desired. This also includes labels for the villager type of each dialogue line. This could be useful for classifying text or creating different text generators for each villager type in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc92656-b60f-4912-8c50-82c32f56dc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue has been saved!\n"
     ]
    }
   ],
   "source": [
    "df_data = {'dialogue':dialogue, 'labels':labels}\n",
    "dialogue_df = pd.DataFrame(df_data)\n",
    "dialogue_df.to_csv('dialogue.csv', index=False)\n",
    "print(\"Dialogue has been saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c28c62a-2ce1-45b0-b43b-3792bf03c19c",
   "metadata": {},
   "source": [
    "Let's check out what our dataframe looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d82e52-fbd3-410f-aa91-25c492b28790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yo, [player]! Whaddya want? [catchphrase]!</td>\n",
       "      <td>cranky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey, hey, [player]! You got somethin' you wann...</td>\n",
       "      <td>cranky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yo, [player]! What're ya doin'? [catchphrase]?</td>\n",
       "      <td>cranky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoa, easy now, [player]. Deep breaths... OK. ...</td>\n",
       "      <td>cranky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh. Hey, [player]. Whaddya want from me? [Catc...</td>\n",
       "      <td>cranky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            dialogue  labels\n",
       "0         Yo, [player]! Whaddya want? [catchphrase]!  cranky\n",
       "1  Hey, hey, [player]! You got somethin' you wann...  cranky\n",
       "2     Yo, [player]! What're ya doin'? [catchphrase]?  cranky\n",
       "3  Whoa, easy now, [player]. Deep breaths... OK. ...  cranky\n",
       "4  Oh. Hey, [player]. Whaddya want from me? [Catc...  cranky"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643be4fb-5c0f-46cf-9d2c-2923cef94644",
   "metadata": {},
   "source": [
    "And let's see how many lines of dialogue we obtained in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fcaef11-7332-46fe-8ef9-6fff76f1b9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd328b39-78a4-419e-ad2b-66eb8f30d22a",
   "metadata": {},
   "source": [
    "It looks like there are just over 600 lines of dialogue on these pages. While it would be ideal to have a greater amount of training data, we can use this as a starting point for generating our dialogue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd9385-a032-4969-b04f-60f95ee19d58",
   "metadata": {},
   "source": [
    "## Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92318076-afcb-4ae9-ae1c-0a30bf23c07c",
   "metadata": {},
   "source": [
    "Unlike humans, computers are not able to understand pure text data. In order to use the lines of dialogue that we obtained, we will need to do some kind of processing to the data. In this case, we will be creating a character based model, so we will process each line of dialogue into a list of character embeddings- a numerical representation of each character that can be used in neural networks.\n",
    "\n",
    "Here are the packages we will need to process the text for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "258495f5-52ae-42ab-9306-30c508a70c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22341a60-5cff-4824-b5af-bf7e0972d454",
   "metadata": {},
   "source": [
    "### Creating character embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760f7d7d-b19e-4da0-96c6-9847d62ba3f3",
   "metadata": {},
   "source": [
    "We will need to create an embedding for each character in our vocabulary. Let's see how many unique characters are contained in these lines of dialogue. Because words like [player] and [item], which represent a variable that can be replaced with the players' name or a specific item in the game respectively, are common in lines of villager dialogue, I chose not to remove symbols like '[' and ']' from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e5cfa3-fb4a-4dbb-9a5d-70d214b94cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 unique characters\n",
      "[' ', '!', '\"', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', ':', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'é', '—', '…', '✮']\n"
     ]
    }
   ],
   "source": [
    "full_text = \"\"\n",
    "\n",
    "for phrase in dialogue_df['dialogue']:\n",
    "    full_text = full_text + ' ' + phrase\n",
    "    \n",
    "full_text = full_text.strip()\n",
    "vocab = sorted(set(full_text))\n",
    "print(f'{len(vocab)} unique characters')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90874806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72034"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261fdc7-22a4-4b27-9600-5a6617969a5c",
   "metadata": {},
   "source": [
    "We'll now split up our dialogue into its individual characters so we can encode them. TensorFlow contains a function to help us do just this, which I use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6964e839-59de-4d05-95ad-39a91ab934e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tf.strings.unicode_split(full_text, input_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891dabc-eb4c-4a2a-b7a4-d8b897d2f8e5",
   "metadata": {},
   "source": [
    "Here's an example of how this encoding looks for our first 10 characters of dialogue. The characters are stored in a 1D tensor object, similar to a default Python list. The 'b' before each character represents the fact the character is a Unicode binary representation of the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f27365c-e8a7-44f2-a8a1-9bbd34129dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'Y' b'o' b',' b' ' b'[' b'p' b'l' b'a' b'y' b'e'], shape=(10,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(chars[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be39f52-5c0f-4510-aaa7-b93cbbfe8c25",
   "metadata": {},
   "source": [
    "Rather than directly processing this text, our model will be using integer ID's to represent each character. Below we create a StringLookup preprocessing layer that we then pass our dialogue strings to in order to encode them as integers. We also create another StringLookup layer that can take the integer ID's the model will output and translate them back to characters.\n",
    "\n",
    "In addition, we can create a text_from_ids function that allows us to quickly pass in a list of ids and translate them into human readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf6a3190-155d-4046-ae30-8cd3fbfbdbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa11fafe-e8ea-477a-bef8-5d429bb0d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ids_from_chars(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "350d022e-e502-4e5e-ac1a-67c6ca77240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dad9d7-13d4-455e-971b-f2c6e79fc4a8",
   "metadata": {},
   "source": [
    "### Creating sequences for training\n",
    "\n",
    "When generating text, our model is actually trying to predict the best (most probable) possible character to come next based on a sequence of characters it has just seen. We need to provide a sequence of a reasonable length: one that is not so long that it is difficult for the model to remember the whole thing, but that is not so short that it will appear extremely frequently in widely varying contexts. In this case, since our pieces of dialogue are relatively short, we will use sequences of 50 characters.\n",
    "\n",
    "We will break up the entire text into sequences, then make sure the model knows how many example sequences to expect each epoch of training after dividing the entire thing up into sequences of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b9a75c3-c563-4779-a957-66a2f66b53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "examples_per_epoch = len(full_text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "178d47c2-7c17-4dfc-b634-62b6ea047fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ea0d8-5f14-4d51-ae8b-1b9998515a02",
   "metadata": {},
   "source": [
    "For each example, the model will predict the next character in the sequence. In addition to the input the model is predicting, we also need to show it the correct answer after it has made its prediction, a target seqence. In this case, our target is simply the input text shifted one character to the right.\n",
    "\n",
    "![](https://www.tensorflow.org/text/tutorials/images/text_generation_sampling.png)\n",
    "\n",
    "Here we'll turn the raw 50 character sequences from our dataset into both input and target sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2d264d1-8bf1-44d4-bd39-ba722f59f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48b83371-3657-487c-8090-dfc62d003ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_seq = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a3d4906-9981-44a1-9844-444b4ac2470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'Yo, [player]! Whaddya want? [catchphrase]! Hey, he'\n",
      "Target: b'o, [player]! Whaddya want? [catchphrase]! Hey, hey'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset_seq.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e560b8",
   "metadata": {},
   "source": [
    "Here we'll finish preparing the data by creating a TensorFlow dataset object that will allow us to fetch samples for our model more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3179422c-18f5-4bf5-82fc-faa234e9c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((64, 50), (64, 50)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "# How many samples we'll see in each batch\n",
    "BATCH_SIZE = 64\n",
    "# How many samples to keep in the memory buffer - we can't fit all of them!\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset_seq\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)) #This lets us prepare upcoming samples in advance\n",
    "\n",
    "#Here we'll check the shape of the input data\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13fd8d8-70ca-45bf-85c0-ec300681c90c",
   "metadata": {},
   "source": [
    "# Building our models\n",
    "\n",
    "To generate potential dialogue snippets, we will be using two different types of RNN cells that are connected to a dense output layer: GRUs and LSTM cells. A RNN (recurrent neural network) is a type of network that is used on sequential data, such as stock market performance data, language, and even music.\n",
    "\n",
    "In addition to generating an output that can be passed to another layer of the network, a recurrent layer also passes its current state forward at each step. GRUs and LSTM cells are types of recurrent layers that also allow for some information to be forgotten or retained for later use by the network. I'll go over the important differences between the two as we train each model.\n",
    "\n",
    "To minimize training time in this toy example, each will contain only a single layer of RNN cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a4816d7-a535-4957-b0b5-7d96a5c77f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some basic information we will need for fitting both models\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension - this determines how large the vector space is for our character embeddings\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d4767",
   "metadata": {},
   "source": [
    "## Our First Model: LSTM\n",
    "\n",
    "LSTM stands for **\"Long-Short Term Memory\"** and is a type of RNN layer that allows the network to forget past information and selectively omit or include information depending on the situation. \n",
    "\n",
    "![A Long-Short Term Memory cell](https://d2l.ai/_images/lstm-0.svg)\n",
    "\n",
    "Because it applies functions to new inputs, outputs of the previous step, and determines what information to retain at what step without any human decision making, this is a fairly computationally intensive type of neural network. Hopefully the quality of predictions will make up for the high compute costs! Let's start building one and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63e78007-83f2-412a-843b-a88412db6dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         20224     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 1024)        5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 79)          80975     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,348,175\n",
      "Trainable params: 5,348,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = tf.keras.Sequential()\n",
    "lstm_model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim))\n",
    "lstm_model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                                return_sequences=True,\n",
    "                                return_state=False))\n",
    "lstm_model.add(tf.keras.layers.Dense(vocab_size))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e78324",
   "metadata": {},
   "source": [
    "This model contains 3 layers: an embedding layer (to turn our integer embeddings into information the neural network can use), our LSTM cells, and a dense layer which serves as our output. We'll check to make sure that it is outputting a prediction of the correct shape before compiling our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b83dfb56-3358-488e-929f-d86b840fc180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 79) lstm_model: (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = lstm_model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"lstm_model: (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180cb36",
   "metadata": {},
   "source": [
    "Before using the model, we need to make sure to compile it. This means assigning it loss function (a value it is trying to minimize) and an optimizer (a function the model will use to help find the best way to minimize the loss). In this case, we will use the ADAM optimizer to try to minimize the Sparse Categorical Cross-Entropy, which is a criteria based on how well our model predicts each class (character in our vocabulary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d53598ef-f24d-461d-9503-5a635861690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67080198-2a5d-4311-92c3-5886f493872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(optimizer='adam', loss=loss, run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d5c753",
   "metadata": {},
   "source": [
    "Now that our model has been compiled, we will train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee19fd27-06c9-43c2-97ae-6d0a159ca252",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50 #You can increase this number for better quality predictions (with longer training)\n",
    "\n",
    "#see if I can add loading onto the GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f861e1f-3d7c-44fb-9dab-ae21f62af505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/22 [==============================] - 58s 3s/step - loss: 3.6630\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 3.2097\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 65s 3s/step - loss: 3.0173\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 89s 4s/step - loss: 2.6733\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 95s 4s/step - loss: 2.4545\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 98s 4s/step - loss: 2.3151\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 101s 5s/step - loss: 2.2186\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 116s 5s/step - loss: 2.1280\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 120s 5s/step - loss: 2.0425\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 126s 6s/step - loss: 1.9681\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 128s 6s/step - loss: 1.9015\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 130s 6s/step - loss: 1.8360\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 144s 7s/step - loss: 1.7729\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 130s 6s/step - loss: 1.7149\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 136s 6s/step - loss: 1.6607\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 137s 6s/step - loss: 1.6114\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 165s 8s/step - loss: 1.5601\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 143s 7s/step - loss: 1.5105\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 147s 7s/step - loss: 1.4635\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 156s 7s/step - loss: 1.4139\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 163s 7s/step - loss: 1.3626\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 157s 7s/step - loss: 1.3140\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 148s 7s/step - loss: 1.2629\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 143s 6s/step - loss: 1.2097\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 162s 7s/step - loss: 1.1612\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 153s 7s/step - loss: 1.1047\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 147s 7s/step - loss: 1.0457\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 154s 7s/step - loss: 0.9847\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 183s 8s/step - loss: 0.9219\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 156s 7s/step - loss: 0.8602\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 146s 7s/step - loss: 0.7942\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 148s 7s/step - loss: 0.7283\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 168s 8s/step - loss: 0.6598\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 151s 7s/step - loss: 0.5969\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 157s 7s/step - loss: 0.5342\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 145s 7s/step - loss: 0.4799\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 142s 6s/step - loss: 0.4224\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 133s 6s/step - loss: 0.3767\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 128s 6s/step - loss: 0.3314\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 118s 5s/step - loss: 0.2951\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 140s 6s/step - loss: 0.2641\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 122s 6s/step - loss: 0.2357\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 119s 5s/step - loss: 0.2121\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 117s 5s/step - loss: 0.1938\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 118s 5s/step - loss: 0.1791\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 122s 6s/step - loss: 0.1677\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 709s 33s/step - loss: 0.1598\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 425s 18s/step - loss: 0.1518\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 249s 11s/step - loss: 0.1447\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 125s 6s/step - loss: 0.1401\n"
     ]
    }
   ],
   "source": [
    "lstm_history = lstm_model.fit(dataset, epochs=EPOCHS) #Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca875e2",
   "metadata": {},
   "source": [
    "Now that the model has been trained, we can use it to generate new dialogue. Here we'll create a separate model class that will allow us to predict a character at a time. We'll concatenate a series of generated characters and reverse their embeddings in order to have human readable dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d15569f1-c904-4202-a1f8-f0282ce6f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits = self.model(inputs=input_ids)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04e7b535-605f-465e-9b2c-af08dfcd0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_lstm = OneStep(lstm_model, chars_from_ids, ids_from_chars, temperature=0.5) \n",
    "# You can adjust the temperature if the desired (anywhere between 0 and 1).\n",
    "# Here I chose a lower temperature which will result in more conservative model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcff50f",
   "metadata": {},
   "source": [
    "Finally, let's see what kind of dialogue we can generate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62a501ec-0cc8-41aa-99af-ab2532427e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow! thtong at'm Thit'e ayou y? to at are t ind che ily? Chr y a te y shit it stoumer theave witint tom yotindanthellondathind wayor t ast ind yor hathpin it Whe thplanor I t t hithithin'sondond an wat t \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.4359660148620605\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "next_char = tf.constant(['Wow']) #Set this string to a starting word or phrase for your dialogue\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(200):\n",
    "    next_char = one_step_lstm.generate_one_step(next_char, states=None)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5b14c",
   "metadata": {},
   "source": [
    "Hmm... it's a little difficult to tell what exactly this model was trying to say here (it certainly doesn't look like any English I've seen). Let's see if another model will produce any better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04fab4c",
   "metadata": {},
   "source": [
    "## A Lighter Model: Using GRUs\n",
    "\n",
    "In addition to our LSTM based model, we will also try creating a model with a layer of GRUs (Gated Recurrent Units) instead. Rather than seperately calculating how much of the input to use, how much to forget, and how much to send to the output of the next cell, a GRU weights the input of the previous step and a newly provided input (\"reset\" and \"update\" gates).\n",
    "\n",
    "![An image of a Gated Recurrent Unit](https://d2l.ai/_images/gru-1.svg)\n",
    "\n",
    "We will use the Model superclass in Keras to create our own new model class used for this second neural network. Like the previous network, it uses an embedding layer for the input and a dense layer to the output. Instead of a recurrent LSTM layer, this time we use a layer of GRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3413d2c8-05ea-44aa-9c7d-2b85874a1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb62a372-b32b-4bda-91a1-be32f00495a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = GRUModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ac33884-5ede-4670-b679-07a0b8c4c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = (\n",
    "    dataset_seq\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a57937",
   "metadata": {},
   "source": [
    "Let's double check the size of our outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "971ce387-3bd1-4b5b-b5d7-cb597fed11d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 79) gru_model: (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset2.take(1):\n",
    "    gru_example_batch_predictions = gru_model(input_example_batch)\n",
    "    print(gru_example_batch_predictions.shape, \"gru_model: (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f7998",
   "metadata": {},
   "source": [
    "And we'll compile this model using the same optimizer and loss function as we did in the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32809c95-a6e6-42b4-8fe8-f3fb4a6b4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ffb520",
   "metadata": {},
   "source": [
    "As before, we'll fit the model and create a OneStep method that will allow us to generate characters from the model one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd80026c-e147-44d7-94c7-e7d3fbe753c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/22 [==============================] - 121s 5s/step - loss: 4.1683\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 63s 3s/step - loss: 3.2333\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 67s 3s/step - loss: 2.8679\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 68s 3s/step - loss: 2.5674\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 68s 3s/step - loss: 2.4080\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 66s 3s/step - loss: 2.3001\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 69s 3s/step - loss: 2.2027\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 69s 3s/step - loss: 2.1082\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 69s 3s/step - loss: 2.0240\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 70s 3s/step - loss: 1.9463\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 72s 3s/step - loss: 1.8754\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 81s 4s/step - loss: 1.8070\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 79s 4s/step - loss: 1.7440\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 73s 3s/step - loss: 1.6828\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 71s 3s/step - loss: 1.6237\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 77s 3s/step - loss: 1.5708\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 76s 3s/step - loss: 1.5144\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 78s 4s/step - loss: 1.4544\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 80s 4s/step - loss: 1.4002\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 88s 4s/step - loss: 1.3449\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 77s 3s/step - loss: 1.2869\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 74s 3s/step - loss: 1.2288\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 74s 3s/step - loss: 1.1704\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 72s 3s/step - loss: 1.1085\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 73s 3s/step - loss: 1.0390\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.9720\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.8968\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 79s 4s/step - loss: 0.8248\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 78s 4s/step - loss: 0.7469\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 88s 4s/step - loss: 0.6716\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.5952\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.5215\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.4544\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.3940\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 74s 3s/step - loss: 0.3412\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 94s 4s/step - loss: 0.2984\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 85s 4s/step - loss: 0.2638\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.2339\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.2122\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 74s 3s/step - loss: 0.1938\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.1804\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.1690\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.1593\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 76s 3s/step - loss: 0.1505\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.1447\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.1393\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.1361\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.1325\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.1300\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.1267\n"
     ]
    }
   ],
   "source": [
    "history2 = gru_model.fit(dataset2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cba717b2-0e6c-44d6-9ac6-cc86a48e01ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_one_step = OneStep(gru_model, chars_from_ids, ids_from_chars, temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d75584",
   "metadata": {},
   "source": [
    "Let's see what we can generate using this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11f5ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow, at thit'se bu ou thitofule t dand thit t me t and ast That m ing thathitouthidout I haserar oume t bu ton athise ar wit t ithe t wanoullatout t at thin'se thit'tid thind win'st t athe t thesthis m t \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.789717435836792\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "next_char = tf.constant(['Wow']) #Set this string to a starting word or phrase for your dialogue\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(200):\n",
    "    next_char = gru_one_step.generate_one_step(next_char, states=None)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c65016",
   "metadata": {},
   "source": [
    "This model didn't produce comprehensable English dialogue either... but this isn't that surprising.\n",
    "\n",
    "While it may be difficult to tell, this model did work as intended. It generates English characters from our vocabulary in a sequence in ways with observable patterns. Unfortunately, these patterns don't quite resemble actual English! Why not?\n",
    "\n",
    "Well... the answer to that is complicated. It is quite time consuming to look inside of each layer and cell of a neural network and understand which variables are affecting what outputs- remember that both of the models above have over 5 million parameters each! There are a few reasons that may be why the networks are not producing the desired output: \n",
    "- **The set of training data could be too small.** Our full text is around 72,000 characters, made into approximately 1400 fifty character sequences. One of the best current human language models, GPT-3, used over 260 billion tokens for its training. More unique training examples would surely improve the performance of these models.\n",
    "- **Our models could need more training to be effective.** Models like GPT-3 and BERT (another highly performing language model) take multiple days to have their parameters sufficiently refined by training. Our models could both be trained in a matter of hours on a CPU (much slower than using parallel processing or even a single GPU). Additional rounds of training (epochs) may help the performance of our model.\n",
    "- **Our network structure could be suboptimal.** Each of our models consistents of an embedding layer, a single recurrent layer, and a dense output layer. They each only have around 5.5 million parameters that go into the function which determines which character to predict next in the sequence. While this may sound like a large number, more effective recent general language models contain not millions or even billions but *trillions* of adjustable parameters. Using additional layers in our network would allow for additional trainable parameters and potentially better predictions (dialogue generation).\n",
    "\n",
    "With these considerations in mind, what would be the best next step to generate dialogue that better resembles English and the style of Animal Crossing?\n",
    "\n",
    "One of the easiest steps to take would be to train the model for a longer amount of time, although its performance would be limited by a small amount of training data. It also could be trained on word embeddings rather than character embeddings. Since the training data contains a limited vocabulary, this is a reasonable way to preprocess our data and would lead to more usage of real words.\n",
    "\n",
    "Some more difficut to implement but likely more effective steps to create a better model would be to use a deeper, more complex model architecture, although this would be limitedly effective without more training data, or to utilize an existing model like GPT-Neo (an open-source version of GPT-3) or BERT. These models often have settings that can be tweaked for particular tasks, allowing users to leverage their powerful capabilities while customizing the flavor of the output.\n",
    "\n",
    "I plan to try some of these strategies, including obtaining additional training data, using word embeddings, and changing our model architecture, in a future notebook. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
